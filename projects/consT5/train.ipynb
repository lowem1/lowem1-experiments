{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/lowem1-experiments/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_16542/194167620.py:26: FutureWarning: set_caching_enabled is deprecated and will be removed in the next major version of datasets. Use datasets.enable_caching() or datasets.disable_caching() instead. This function will be removed in a future version of datasets.\n",
      "  set_caching_enabled(False)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModel,\n",
    "    DefaultDataCollator,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset, Dataset, DatasetDict,set_caching_enabled, concatenate_datasets\n",
    "import nlpaug.augmenter.char as nac\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import set_caching_enabled\n",
    "set_caching_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiments.yml\") as stream:\n",
    "  runtime_conf = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_vars = runtime_conf[\"global\"]\n",
    "experiment_params = runtime_conf[\"training\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = global_vars[\"experiment_name\"]\n",
    "N_SAMPLES = global_vars['n_samples']\n",
    "N_AUGS = global_vars['n_augs']\n",
    "AUG_PARAMS = global_vars[\"augmentation_params\"]\n",
    "CHECKPOINT = global_vars[\"checkpoint\"]\n",
    "LABELS = global_vars[\"labels\"]\n",
    "MAX_LENGTH = global_vars[\"max_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(CHECKPOINT, num_labels=LABELS, max_length=MAX_LENGTH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT,max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_noisy_lines(row, src_col: str) -> list:\n",
    "    aug = nac.OcrAug(AUG_PARAMS)\n",
    "    sentence = row[src_col]\n",
    "    aug_sents = aug.augment(sentence)[0]\n",
    "    return dict(text=sentence,tgt_col=aug_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_mapped_encodings(row, src_col: str, aug_col: str) -> dict:\n",
    "\n",
    "  src = tokenizer.batch_encode_plus(\n",
    "      row[src_col],\n",
    "      truncation=True,\n",
    "      padding=\"max_length\",\n",
    "      max_length=128,\n",
    "      return_tensors=\"pt\"\n",
    "  )\n",
    "\n",
    "  aug = tokenizer.batch_encode_plus(\n",
    "      row[aug_col],\n",
    "      truncation=True,\n",
    "      padding=\"max_length\",\n",
    "      max_length=128,\n",
    "      return_tensors=\"pt\"\n",
    "  )\n",
    "\n",
    "  return dict(\n",
    "\n",
    "      input_ids=aug.input_ids,\n",
    "      attention_mask=aug.attention_mask,\n",
    "      decoder_attention_mask=src.attention_mask,\n",
    "      labels=src.input_ids\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_feature_extraction(data: Dataset, num_perms: int=1) -> DatasetDict:\n",
    "  # generate augmented OCR noise and get input ids for byte tokens\n",
    "  dataset_builder: list = []\n",
    "  for i in range(num_perms):\n",
    "    tmp  = data.map(with_noisy_lines, fn_kwargs=dict(src_col=\"text\"))\n",
    "    dataset_builder.append(tmp)\n",
    "  dataset = concatenate_datasets(dataset_builder)\n",
    "  dataset = dataset.map(\n",
    "      with_mapped_encodings, fn_kwargs=dict(src_col=\"text\", aug_col=\"tgt_col\"),\n",
    "      batched=True,\n",
    "    )\n",
    "  # reformat for training/inference\n",
    "  dataset = dataset.remove_columns([\"text\", \"tgt_col\"])\n",
    "  dataset = dataset.with_format(type='torch')\n",
    "  return dataset.train_test_split(test_size=0.2).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 511/511 [00:00<00:00, 2.14MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /home/vscode/.cache/huggingface/datasets/lowem1___parquet/lowem1--training-invoices-e93c374bc59d64cc/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 6.49k/6.49k [00:00<00:00, 10.2MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 673.35it/s]\n",
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/vscode/.cache/huggingface/datasets/lowem1___parquet/lowem1--training-invoices-e93c374bc59d64cc/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 28.98it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"lowem1/training-invoices\")[\"train\"]\n",
    "dataset = dataset.rename_column(\"line_data\", \"text\")\n",
    "dataset = dataset.rename_column(\"label\", \"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train,val = with_feature_extraction(dataset,num_perms=N_AUGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir ./logs  --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, args in experiment_params.items():\n",
    "#   training_args = TrainingArguments(**args[\"training_args\"])\n",
    "#   torch.cuda.empty_cache()\n",
    "#   trainer =  Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train,\n",
    "#     eval_dataset=val,\n",
    "#     callbacks=[TensorBoardCallback(SummaryWriter(log_dir=f\"./logs/{name}\"))]\n",
    "#   )\n",
    "#   trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
