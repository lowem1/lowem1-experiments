{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/lowem1-experiments/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_2650/194167620.py:26: FutureWarning: set_caching_enabled is deprecated and will be removed in the next major version of datasets. Use datasets.enable_caching() or datasets.disable_caching() instead. This function will be removed in a future version of datasets.\n",
      "  set_caching_enabled(False)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModel,\n",
    "    DefaultDataCollator,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset, Dataset, DatasetDict,set_caching_enabled, concatenate_datasets\n",
    "import nlpaug.augmenter.char as nac\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import set_caching_enabled\n",
    "set_caching_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiments.yml\") as stream:\n",
    "  runtime_conf = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_vars = runtime_conf[\"global\"]\n",
    "experiment_params = runtime_conf[\"training\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = global_vars[\"experiment_name\"]\n",
    "MODE = global_vars[\"mode\"]\n",
    "BEST_RUN = global_vars[\"best_run\"]\n",
    "N_SAMPLES = global_vars['n_samples']\n",
    "N_AUGS = global_vars['n_augs']\n",
    "AUG_PARAMS = global_vars[\"augmentation_params\"]\n",
    "CHECKPOINT = global_vars[\"checkpoint\"]\n",
    "LABELS = global_vars[\"labels\"]\n",
    "MAX_LENGTH = global_vars[\"max_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(CHECKPOINT, num_labels=LABELS, max_length=MAX_LENGTH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT,max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_noisy_lines(row, src_col: str) -> list:\n",
    "    aug = nac.OcrAug(AUG_PARAMS)\n",
    "    sentence = row[src_col]\n",
    "    aug_sents = aug.augment(sentence)[0]\n",
    "    return dict(text=sentence,tgt_col=aug_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_mapped_encodings(row, src_col: str, aug_col: str) -> dict:\n",
    "\n",
    "  src = tokenizer.batch_encode_plus(\n",
    "      row[src_col],\n",
    "      truncation=True,\n",
    "      padding=\"max_length\",\n",
    "      max_length=128,\n",
    "      return_tensors=\"pt\"\n",
    "  )\n",
    "\n",
    "  aug = tokenizer.batch_encode_plus(\n",
    "      row[aug_col],\n",
    "      truncation=True,\n",
    "      padding=\"max_length\",\n",
    "      max_length=128,\n",
    "      return_tensors=\"pt\"\n",
    "  )\n",
    "\n",
    "  return dict(\n",
    "\n",
    "      input_ids=aug.input_ids,\n",
    "      attention_mask=aug.attention_mask,\n",
    "      decoder_attention_mask=src.attention_mask,\n",
    "      labels=src.input_ids\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_feature_extraction(data: Dataset, num_perms: int=1) -> DatasetDict:\n",
    "  # generate augmented OCR noise and get input ids for byte tokens\n",
    "  dataset_builder: list = []\n",
    "  for i in range(num_perms):\n",
    "    tmp  = data.map(with_noisy_lines, fn_kwargs=dict(src_col=\"text\"))\n",
    "    dataset_builder.append(tmp)\n",
    "  dataset = concatenate_datasets(dataset_builder)\n",
    "  dataset = dataset.map(\n",
    "      with_mapped_encodings, fn_kwargs=dict(src_col=\"text\", aug_col=\"tgt_col\"),\n",
    "      batched=True,\n",
    "    )\n",
    "  # reformat for training/inference\n",
    "  dataset = dataset.remove_columns([\"text\", \"tgt_col\"])\n",
    "  dataset = dataset.with_format(type='torch')\n",
    "  return dataset.train_test_split(test_size=0.2).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 511/511 [00:00<00:00, 2.14MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /home/vscode/.cache/huggingface/datasets/lowem1___parquet/lowem1--training-invoices-e93c374bc59d64cc/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 6.49k/6.49k [00:00<00:00, 10.2MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 673.35it/s]\n",
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/vscode/.cache/huggingface/datasets/lowem1___parquet/lowem1--training-invoices-e93c374bc59d64cc/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 28.98it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"lowem1/training-invoices\")[\"train\"]\n",
    "dataset = dataset.rename_column(\"line_data\", \"text\")\n",
    "dataset = dataset.rename_column(\"label\", \"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train,val = with_feature_extraction(dataset,num_perms=N_AUGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir ./logs  --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=50,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=invoice_lr_3/runs/Jun30_18-53-55_fac747fdefe6,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=50,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=invoice_lr_3,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=invoice_lr_3,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if MODE == \"deploy\":\n",
    "    experiements = dict(best_run=experiment_params[BEST_RUN])\n",
    "    \n",
    "for name, args in experiements.items():\n",
    "    training_args = TrainingArguments(**args[\"training_args\"])\n",
    "    print(training_args)\n",
    "    torch.cuda.empty_cache()\n",
    "    trainer =  Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    callbacks=[TensorBoardCallback(SummaryWriter(log_dir=f\"./logs/{name}\"))]\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment_0': {'training_args': {'output_dir': 'invoice_lr_1',\n",
       "   'evaluation_strategy': 'steps',\n",
       "   'num_train_epochs': 1,\n",
       "   'learning_rate': 0.1,\n",
       "   'per_device_train_batch_size': 16,\n",
       "   'per_device_eval_batch_size': 16,\n",
       "   'logging_steps': 50}},\n",
       " 'experiment_1': {'training_args': {'output_dir': 'invoice_lr_2',\n",
       "   'evaluation_strategy': 'steps',\n",
       "   'num_train_epochs': 1,\n",
       "   'learning_rate': 0.01,\n",
       "   'per_device_train_batch_size': 16,\n",
       "   'per_device_eval_batch_size': 16,\n",
       "   'logging_steps': 50}},\n",
       " 'experiment_2': {'training_args': {'output_dir': 'invoice_lr_3',\n",
       "   'evaluation_strategy': 'steps',\n",
       "   'num_train_epochs': 1,\n",
       "   'learning_rate': 0.001,\n",
       "   'per_device_train_batch_size': 16,\n",
       "   'per_device_eval_batch_size': 16,\n",
       "   'logging_steps': 50}},\n",
       " 'experiment_3': {'training_args': {'output_dir': 'invoice_lr_4',\n",
       "   'evaluation_strategy': 'steps',\n",
       "   'num_train_epochs': 1,\n",
       "   'learning_rate': 0.0001,\n",
       "   'per_device_train_batch_size': 16,\n",
       "   'per_device_eval_batch_size': 16,\n",
       "   'logging_steps': 50}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
